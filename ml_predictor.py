#!/usr/bin/env python3
"""
Comprehensive Fixed ML Module for PhishGuard v3.1.2
Fixes all false positives including educational and government domains
"""

import json
import logging
import os
import re
import numpy as np
from pathlib import Path
from typing import Dict, Tuple, Optional
from urllib.parse import urlparse

# Set up logging
logger = logging.getLogger(__name__)

# Feature extraction functions
suspicious_keywords = [
    "login", "secure", "account", "update", "bank", "paypal", "verify", 
    "confirm", "signin", "password", "suspended", "limited", "expire",
    "urgent", "immediate", "click", "here", "now", "free", "winner"
]

# üõ°Ô∏è COMPREHENSIVE TRUSTED DOMAINS - Major Sites
TRUSTED_DOMAINS = {
    'google.com', 'facebook.com', 'twitter.com', 'instagram.com', 'linkedin.com',
    'amazon.com', 'microsoft.com', 'apple.com', 'github.com', 'stackoverflow.com',
    'youtube.com', 'gmail.com', 'outlook.com', 'yahoo.com', 'reddit.com',
    'wikipedia.org', 'mozilla.org', 'github.io', 'cloudflare.com', 'netflix.com',
    'spotify.com', 'dropbox.com', 'adobe.com', 'salesforce.com', 'zoom.us',
    'discord.com', 'slack.com', 'twitch.tv', 'medium.com', 'wordpress.com'
}

# üèõÔ∏è GOVERNMENT AND EDUCATIONAL TLD PATTERNS
TRUSTED_TLD_PATTERNS = [
    # Educational domains
    r'\.edu$',           # US educational
    r'\.edu\.[a-z]{2}$', # International educational (edu.my, edu.au, etc.)
    r'\.ac\.[a-z]{2}$',  # Academic domains (ac.uk, ac.nz, etc.)
    r'\.univ-[a-z]+\.fr$', # French universities
    
    # Government domains  
    r'\.gov$',           # US government
    r'\.gov\.[a-z]{2}$', # International government (gov.my, gov.au, etc.)
    r'\.mil$',           # Military
    r'\.mil\.[a-z]{2}$', # International military
    
    # Specific country educational/government patterns
    r'\.uthm\.edu\.my$', # UTHM Malaysia (your specific case!)
    r'\.utm\.my$',       # UTM Malaysia
    r'\.upm\.edu\.my$',  # UPM Malaysia
    r'\.um\.edu\.my$',   # University of Malaya
]

# üá≤üáæ MALAYSIAN EDUCATIONAL INSTITUTIONS (specific for your case)
MALAYSIAN_EDU_DOMAINS = {
    'uthm.edu.my',  # Universiti Tun Hussein Onn Malaysia (YOUR CASE!)
    'utm.my',       # Universiti Teknologi Malaysia
    'upm.edu.my',   # Universiti Putra Malaysia
    'um.edu.my',    # University of Malaya
    'usm.my',       # Universiti Sains Malaysia
    'ukm.my',       # Universiti Kebangsaan Malaysia
    'utp.edu.my',   # Universiti Teknologi PETRONAS
    'mmu.edu.my',   # Multimedia University
}

class PhishingPredictor:
    """Enhanced phishing prediction with comprehensive domain recognition"""
    
    def __init__(self, model_path: str = "model/xgboost_model.json"):
        self.model_path = model_path
        self.model = None
        self.model_loaded = False
        self.feature_names = ["URL_Length", "Special_Chars", "Num_Subdomains", "Suspicious_Keywords", "Has_HTTPS"]
        
        # Try to load XGBoost model
        self._load_xgboost_model()
    
    def _load_xgboost_model(self):
        """Load XGBoost model from JSON file"""
        try:
            if not os.path.exists(self.model_path):
                logger.warning(f"XGBoost model not found at {self.model_path}. Using rule-based fallback.")
                return
            
            # Import XGBoost
            import xgboost as xgb
            
            # Load the model
            self.model = xgb.Booster()
            self.model.load_model(self.model_path)
            self.model_loaded = True
            
            logger.info("‚úÖ XGBoost model loaded successfully")
            
        except ImportError:
            logger.warning("XGBoost not installed. Using rule-based prediction.")
        except Exception as e:
            logger.error(f"Failed to load XGBoost model: {e}")
            logger.info("Falling back to rule-based prediction")
    
    def extract_features(self, url: str) -> Dict[str, float]:
        """Extract features from URL for ML prediction"""
        try:
            parsed = urlparse(url)
            
            # Basic URL features
            url_length = len(url)
            special_chars = sum(url.count(c) for c in "@/?=_-&%#")
            
            # Subdomain analysis
            if parsed.netloc:
                num_subdomains = max(0, parsed.netloc.count('.') - 1)
            else:
                num_subdomains = 0
            
            # Suspicious keywords
            url_lower = url.lower()
            suspicious_count = sum(1 for keyword in suspicious_keywords if keyword in url_lower)
            
            # HTTPS check
            has_https = 1.0 if parsed.scheme == "https" else 0.0
            
            features = {
                "URL_Length": float(url_length),
                "Special_Chars": float(special_chars),
                "Num_Subdomains": float(num_subdomains),
                "Suspicious_Keywords": float(suspicious_count),
                "Has_HTTPS": has_https
            }
            
            return features
            
        except Exception as e:
            logger.error(f"Feature extraction error: {e}")
            # Return default features
            return {
                "URL_Length": 0.0,
                "Special_Chars": 0.0,
                "Num_Subdomains": 0.0,
                "Suspicious_Keywords": 0.0,
                "Has_HTTPS": 0.0
            }
    
    def is_trusted_domain(self, url: str) -> Tuple[bool, str]:
        """Check if URL belongs to a trusted domain with reason"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            original_domain = domain
            
            # Remove common prefixes
            prefixes_to_remove = ['www.', 'mail.', 'webmail.', 'm.', 'mobile.', 'app.', 'api.']
            for prefix in prefixes_to_remove:
                if domain.startswith(prefix):
                    domain = domain[len(prefix):]
                    break
            
            # Remove port number
            domain = domain.split(':')[0]
            
            # 1. Check major trusted domains
            for trusted in TRUSTED_DOMAINS:
                if domain == trusted or domain.endswith('.' + trusted):
                    return True, f"Major trusted domain: {trusted}"
            
            # 2. Check Malaysian educational institutions (YOUR SPECIFIC CASE!)
            for edu_domain in MALAYSIAN_EDU_DOMAINS:
                if domain == edu_domain or domain.endswith('.' + edu_domain):
                    return True, f"Malaysian educational institution: {edu_domain}"
            
            # 3. Check educational and government TLD patterns
            for pattern in TRUSTED_TLD_PATTERNS:
                if re.search(pattern, domain):
                    return True, f"Educational/Government domain pattern: {pattern}"
            
            # 4. Check for government domains (.gov, .mil)
            if domain.endswith('.gov') or domain.endswith('.mil'):
                return True, "Government domain"
            
            # 5. Check for educational domains (.edu)
            if domain.endswith('.edu'):
                return True, "Educational domain"
            
            # 6. Check for academic domains (.ac.xx)
            if '.ac.' in domain:
                return True, "Academic domain"
            
            # 7. Check for specific university patterns
            university_keywords = ['university', 'univ', 'college', 'school', 'academy']
            if any(keyword in domain for keyword in university_keywords):
                # Be more lenient with educational-sounding domains
                if not any(suspicious in domain for suspicious in ['login', 'secure', 'verify', 'update']):
                    return True, "Educational-sounding domain"
            
            return False, "Not in trusted lists"
            
        except Exception as e:
            logger.error(f"Trusted domain check error: {e}")
            return False, "Error in domain check"
    
    def predict_with_xgboost(self, features: Dict[str, float], url: str) -> Tuple[str, float, int]:
        """Make prediction using XGBoost model"""
        try:
            import xgboost as xgb
            
            # Check trusted domains first - HIGHEST PRIORITY
            is_trusted, reason = self.is_trusted_domain(url)
            if is_trusted:
                logger.info(f"‚úÖ Trusted domain detected: {reason}")
                return "Safe", 97.0, 3
            
            # Prepare features in correct order
            feature_array = np.array([[features[name] for name in self.feature_names]])
            
            # Create DMatrix
            dmatrix = xgb.DMatrix(feature_array, feature_names=self.feature_names)
            
            # Get prediction probability (probability of being phishing)
            phishing_probability = self.model.predict(dmatrix)[0]
            
            # Apply more conservative threshold for XGBoost
            if phishing_probability > 0.7:  # Higher threshold
                label = "Phishing"
                confidence = float(phishing_probability * 100)
                risk_score = int(phishing_probability * 100)
            elif phishing_probability > 0.3:  # Uncertain range - lean safe
                label = "Safe"
                confidence = float((1 - phishing_probability) * 85)  # Lower confidence for uncertain
                risk_score = int(phishing_probability * 100)
            else:
                label = "Safe"
                confidence = float((1 - phishing_probability) * 100)
                risk_score = int(phishing_probability * 100)
            
            return label, confidence, risk_score
            
        except Exception as e:
            logger.error(f"XGBoost prediction error: {e}")
            return self.predict_with_rules(features, url)
    
    def predict_with_rules(self, features: Dict[str, float], url: str) -> Tuple[str, float, int]:
        """Enhanced rule-based prediction with comprehensive domain recognition"""
        
        # Check trusted domains first - HIGHEST PRIORITY
        is_trusted, reason = self.is_trusted_domain(url)
        if is_trusted:
            logger.info(f"‚úÖ Trusted domain detected: {reason}")
            return "Safe", 98.0, 2
        
        score = 0
        penalties = []
        
        # Extract feature values
        url_length = features.get("URL_Length", 0)
        special_chars = features.get("Special_Chars", 0)
        suspicious_keywords = features.get("Suspicious_Keywords", 0)
        has_https = features.get("Has_HTTPS", 1)
        num_subdomains = features.get("Num_Subdomains", 0)
        
        # MORE CONSERVATIVE LENGTH-BASED SCORING
        if url_length > 150:  # Much higher threshold
            score += 30
            penalties.append(f"Very long URL: {url_length} chars")
        elif url_length > 100:
            score += 20
            penalties.append(f"Long URL: {url_length} chars")
        elif url_length > 80:
            score += 10
            penalties.append(f"Moderately long URL: {url_length} chars")
        
        # MORE CONSERVATIVE SPECIAL CHARACTERS
        special_char_ratio = special_chars / max(url_length, 1)
        if special_char_ratio > 0.4:  # Very high ratio
            score += 25
            penalties.append(f"High special char ratio: {special_char_ratio:.2f}")
        elif special_chars > 20:  # High absolute count
            score += 20
            penalties.append(f"Many special chars: {special_chars}")
        elif special_chars > 15:
            score += 10
            penalties.append(f"Some special chars: {special_chars}")
        
        # SUSPICIOUS KEYWORDS - Most important indicator
        if suspicious_keywords >= 4:
            score += 40
            penalties.append(f"Many suspicious keywords: {suspicious_keywords}")
        elif suspicious_keywords >= 3:
            score += 30
            penalties.append(f"Several suspicious keywords: {suspicious_keywords}")
        elif suspicious_keywords >= 2:
            score += 20
            penalties.append(f"Some suspicious keywords: {suspicious_keywords}")
        elif suspicious_keywords >= 1:
            score += 10
            penalties.append(f"One suspicious keyword: {suspicious_keywords}")
        
        # HTTPS CHECK
        if has_https == 0:
            score += 20
            penalties.append("No HTTPS encryption")
        
        # MORE LENIENT SUBDOMAIN ANALYSIS
        if num_subdomains > 6:  # Much higher threshold
            score += 20
            penalties.append(f"Excessive subdomains: {num_subdomains}")
        elif num_subdomains > 4:
            score += 10
            penalties.append(f"Many subdomains: {num_subdomains}")
        # Note: 2-3 subdomains are normal for many legitimate sites
        
        # Additional checks for malicious patterns
        url_lower = url.lower()
        
        # Check for IP addresses instead of domains
        if re.search(r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b', url):
            score += 25
            penalties.append("IP address used instead of domain")
        
        # Check for suspicious TLDs
        suspicious_tlds = ['.tk', '.ml', '.ga', '.cf', '.click', '.download']
        if any(tld in url_lower for tld in suspicious_tlds):
            score += 15
            penalties.append("Suspicious TLD")
        
        # Check for homograph attacks
        if any(ord(c) > 127 for c in url):
            score += 15
            penalties.append("Non-ASCII characters detected")
        
        # Final scoring with MUCH MORE CONSERVATIVE THRESHOLD
        risk_score = min(score, 100)
        
        # INCREASED THRESHOLD: Must score 70+ to be considered phishing
        if risk_score >= 70:
            label = "Phishing"
            confidence = min(95.0, risk_score)
        elif risk_score >= 50:  # Moderate risk - still classify as safe but lower confidence
            label = "Safe"
            confidence = min(70.0, 100 - risk_score)
        else:
            label = "Safe"
            confidence = min(95.0, 100 - risk_score + 10)
        
        if penalties:
            logger.info(f"Applied penalties: {', '.join(penalties)}")
        
        return label, confidence, risk_score
    
    def predict(self, url: str) -> Tuple[str, float, int, Dict[str, float]]:
        """Main prediction function with enhanced domain recognition"""
        try:
            # Extract features
            features = self.extract_features(url)
            
            # Use XGBoost if available, otherwise enhanced rules
            if self.model_loaded and self.model is not None:
                label, confidence, risk_score = self.predict_with_xgboost(features, url)
                model_type = "XGBoost"
            else:
                label, confidence, risk_score = self.predict_with_rules(features, url)
                model_type = "Enhanced Rule-based"
            
            # Log prediction with domain analysis
            is_trusted, reason = self.is_trusted_domain(url)
            if is_trusted:
                logger.info(f"{model_type} prediction: {url} ‚Üí {label} ({confidence:.1f}%) - {reason}")
            else:
                logger.info(f"{model_type} prediction: {url} ‚Üí {label} ({confidence:.1f}%, Risk: {risk_score}%)")
            
            return label, confidence, risk_score, features
            
        except Exception as e:
            logger.error(f"Prediction error for {url}: {e}")
            # Return safe default
            default_features = self.extract_features(url)
            return "Safe", 75.0, 25, default_features
    
    def get_model_info(self) -> Dict[str, any]:
        """Get comprehensive model information"""
        return {
            "model_type": "XGBoost (Enhanced v3.1.2)" if self.model_loaded else "Rule-based (Enhanced v3.1.2)",
            "model_loaded": self.model_loaded,
            "model_path": self.model_path,
            "feature_names": self.feature_names,
            "model_exists": os.path.exists(self.model_path),
            "trusted_domains": len(TRUSTED_DOMAINS),
            "malaysian_edu_domains": len(MALAYSIAN_EDU_DOMAINS),
            "trusted_tld_patterns": len(TRUSTED_TLD_PATTERNS),
            "version": "3.1.2-comprehensive",
            "fixes": [
                "Educational domain recognition (.edu, .ac)",
                "Government domain recognition (.gov, .mil)",
                "Malaysian educational institutions",
                "Conservative scoring thresholds",
                "Enhanced subdomain analysis",
                "IP address detection",
                "Suspicious TLD detection"
            ]
        }

# Global predictor instance
_predictor = None

def get_predictor() -> PhishingPredictor:
    """Get or create global predictor instance"""
    global _predictor
    if _predictor is None:
        _predictor = PhishingPredictor()
    return _predictor

def predict_url(url: str) -> Tuple[str, float, int, Dict[str, float]]:
    """Convenience function for URL prediction"""
    predictor = get_predictor()
    return predictor.predict(url)

def extract_features(url: str) -> Dict[str, float]:
    """Convenience function for feature extraction"""
    predictor = get_predictor()
    return predictor.extract_features(url)

def get_model_status() -> Dict[str, any]:
    """Get model status information"""
    predictor = get_predictor()
    return predictor.get_model_info()

def test_educational_domains():
    """Test specifically for educational domain fixes"""
    test_urls = [
        # Your specific case
        "https://smap.uthm.edu.my",
        
        # Other Malaysian educational institutions
        "https://portal.utm.my",
        "https://student.upm.edu.my",
        "https://library.um.edu.my",
        
        # International educational domains
        "https://student.mit.edu",
        "https://portal.stanford.edu",
        "https://blackboard.ox.ac.uk",
        "https://moodle.cam.ac.uk",
        
        # Government domains
        "https://portal.gov.my",
        "https://irs.gov",
        
        # Major trusted sites (should still work)
        "https://google.com",
        "https://github.com",
        
        # Suspicious URLs (should still be caught)
        "https://google-security-update.suspicious-domain.tk",
        "https://paypal-verification.fake-site.ml"
    ]
    
    predictor = get_predictor()
    print("üß™ COMPREHENSIVE DOMAIN TESTING")
    print("=" * 80)
    print(f"Model: {predictor.get_model_info()['model_type']}")
    print("=" * 80)
    
    for url in test_urls:
        label, confidence, risk_score, features = predictor.predict(url)
        is_trusted, reason = predictor.is_trusted_domain(url)
        
        if label == "Safe":
            status = "‚úÖ"
        else:
            status = "‚ö†Ô∏è"
        
        print(f"\n{status} {url}")
        print(f"   Prediction: {label} ({confidence:.1f}%)")
        print(f"   Risk Score: {risk_score}%")
        if is_trusted:
            print(f"   üõ°Ô∏è Trusted: {reason}")
        print("-" * 60)

if __name__ == "__main__":
    print("üõ°Ô∏è PhishGuard Enhanced ML Predictor v3.1.2")
    print("üéØ Comprehensive fix for educational and government domains")
    test_educational_domains()